{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb3f14a",
   "metadata": {},
   "source": [
    "# PPS Workbench (v1.0)\n",
    "Convert a one-row-per-pair spreadsheet into the v1 JSON dataset, validate it, and try simple queries.\n",
    "- **Input:** CSV/XLSX with columns: `id, ref, chapter, text_KJV, text_WEB(optional), input_tag, output_tag, type, anchor1, anchor2, note`\n",
    "- **Output:** `/data/proverbs.json` grouped by verse/range with `pairs[]`.\n",
    "- **Rules (v1.0):** positive, unambiguous **causal** pairs only; KJV (default) + WEB display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfecf115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Imports & paths\n",
    "import os, json, re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "BASE = \"/mnt/data\"\n",
    "DATA_DIR = os.path.join(BASE, \"pps_docs_stub\", \"data\")\n",
    "INDEX_DIR = os.path.join(DATA_DIR, \"indexes\")\n",
    "CONFIG_PATH = os.path.join(DATA_DIR, \"config.json\")\n",
    "PROVERBS_JSON_PATH = os.path.join(DATA_DIR, \"proverbs.json\")\n",
    "INPUT_PATH = \"\"  # <-- set to your CSV/XLSX path (uploaded to this runtime), e.g., '/mnt/data/your_file.xlsx'\n",
    "\n",
    "print(\"Using data dir:\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load input sheet\n",
    "if not INPUT_PATH:\n",
    "    print(\"Set INPUT_PATH to your CSV/XLSX file path and re-run this cell.\")\n",
    "else:\n",
    "    if INPUT_PATH.lower().endswith(\".csv\"):\n",
    "        raw = pd.read_csv(INPUT_PATH)\n",
    "    else:\n",
    "        raw = pd.read_excel(INPUT_PATH)\n",
    "    display(raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7c2a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Normalize & basic checks\n",
    "required_cols = [\"id\",\"ref\",\"chapter\",\"text_KJV\",\"input_tag\",\"output_tag\",\"type\"]\n",
    "optional_cols = [\"text_WEB\",\"anchor1\",\"anchor2\",\"note\"]\n",
    "\n",
    "def normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Ensure columns exist\n",
    "    for c in required_cols + optional_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"\"\n",
    "    # Strip whitespace\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    # Coerce chapter to int where possible\n",
    "    def to_int(x):\n",
    "        try:\n",
    "            return int(float(x))\n",
    "        except Exception:\n",
    "            return None\n",
    "    df[\"chapter\"] = df[\"chapter\"].apply(to_int)\n",
    "    return df[required_cols + optional_cols]\n",
    "\n",
    "def validate_rows(df: pd.DataFrame):\n",
    "    id_re = re.compile(r\"^Prov\\.\\d{2}\\.\\d{2}(-\\d{2})?$\")\n",
    "    ref_re = re.compile(r\"^Proverbs\\s+\\d+:\\d+(-\\d+)?$\")\n",
    "    errors = []\n",
    "    for i, row in df.iterrows():\n",
    "        row_errs = []\n",
    "        if not id_re.match(str(row[\"id\"])):\n",
    "            row_errs.append(\"bad id\")\n",
    "        if not ref_re.match(str(row[\"ref\"])):\n",
    "            row_errs.append(\"bad ref\")\n",
    "        chap = row[\"chapter\"]\n",
    "        if chap is None or chap < 1 or chap > 31:\n",
    "            row_errs.append(\"bad chapter\")\n",
    "        if not row[\"text_KJV\"]:\n",
    "            row_errs.append(\"empty text_KJV\")\n",
    "        if str(row[\"type\"]).lower() != \"causal\":\n",
    "            row_errs.append(\"type must be 'causal' for v1\")\n",
    "        if not row[\"input_tag\"]:\n",
    "            row_errs.append(\"missing input_tag\")\n",
    "        if not row[\"output_tag\"]:\n",
    "            row_errs.append(\"missing output_tag\")\n",
    "        for a in [\"anchor1\",\"anchor2\"]:\n",
    "            if row[a] and len(str(row[a])) > 120:\n",
    "                row_errs.append(f\"{a} too long (>120)\")\n",
    "        if row_errs:\n",
    "            errors.append({\"row\": int(i), \"issues\": row_errs})\n",
    "    return errors\n",
    "\n",
    "if 'raw' in globals():\n",
    "    df = normalize_df(raw.copy())\n",
    "    errs = validate_rows(df)\n",
    "    print(f\"Rows: {len(df)}  |  Errors: {len(errs)}\")\n",
    "    if errs:\n",
    "        display(pd.DataFrame(errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f6e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Derive canonical tag lists (or load separately if you prefer)\n",
    "def derive_tags(df: pd.DataFrame):\n",
    "    inputs = sorted(set(df[\"input_tag\"].dropna().astype(str)))\n",
    "    outputs = sorted(set(df[\"output_tag\"].dropna().astype(str)))\n",
    "    input_tags = [{\"id\": t, \"label\": t.replace(\"_\",\" \").title()} for t in inputs if t]\n",
    "    output_tags = [{\"id\": t, \"label\": t.replace(\"_\",\" \").title()} for t in outputs if t]\n",
    "    return input_tags, output_tags\n",
    "\n",
    "if 'df' in globals():\n",
    "    input_tags, output_tags = derive_tags(df)\n",
    "    print(f\"Derived {len(input_tags)} input tags, {len(output_tags)} output tags\")\n",
    "    # Save for the site\n",
    "    with open(os.path.join(DATA_DIR, \"input_tags.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(input_tags, f, ensure_ascii=False, indent=2)\n",
    "    with open(os.path.join(DATA_DIR, \"output_tags.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_tags, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe92c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Group into verse-level JSON\n",
    "def group_rows(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    grouped = {}\n",
    "    for _, r in df.iterrows():\n",
    "        rid = r[\"id\"]\n",
    "        if rid not in grouped:\n",
    "            grouped[rid] = {\n",
    "                \"id\": rid,\n",
    "                \"ref\": r[\"ref\"],\n",
    "                \"text_KJV\": r[\"text_KJV\"],\n",
    "                \"text_WEB\": r.get(\"text_WEB\",\"\") or None,\n",
    "                \"pairs\": [],\n",
    "                \"note\": r.get(\"note\",\"\") or None\n",
    "            }\n",
    "        pair = {\n",
    "            \"input_tag\": r[\"input_tag\"],\n",
    "            \"output_tag\": r[\"output_tag\"],\n",
    "            \"type\": \"causal\",\n",
    "            \"anchor_phrases\": [p for p in [r.get(\"anchor1\",\"\"), r.get(\"anchor2\",\"\")] if p]\n",
    "        }\n",
    "        grouped[rid][\"pairs\"].append(pair)\n",
    "    # Sort by id Prov.CC.VV\n",
    "    def sort_key(e):\n",
    "        # handle ranges: Prov.CC.VV or Prov.CC.VV-VV\n",
    "        parts = e.split(\".\")\n",
    "        cc, vv = parts[1], parts[2]\n",
    "        vv0 = vv.split(\"-\")[0]\n",
    "        return (int(cc), int(vv0))\n",
    "    items = [grouped[k] for k in sorted(grouped.keys(), key=sort_key)]\n",
    "    return {\n",
    "        \"meta\": {\"book\": \"Proverbs\", \"version\": \"1.0\"},\n",
    "        \"data\": items\n",
    "    }\n",
    "\n",
    "if 'df' in globals() and not errs:\n",
    "    dataset = group_rows(df)\n",
    "    with open(PROVERBS_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
    "    print(\"Wrote:\", PROVERBS_JSON_PATH)\n",
    "    print(\"Records:\", len(dataset[\"data\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf89bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Build tiny indexes (optional)\n",
    "def build_indexes(dataset: Dict[str, Any]):\n",
    "    by_tag = {}\n",
    "    by_chap = {}\n",
    "    for rec in dataset[\"data\"]:\n",
    "        # chapter inferred from ref \"Proverbs C:V(-VV)\"\n",
    "        try:\n",
    "            chap = int(rec[\"ref\"].split()[1].split(\":\")[0])\n",
    "        except Exception:\n",
    "            chap = None\n",
    "        if chap:\n",
    "            by_chap.setdefault(str(chap), []).append(rec[\"id\"])\n",
    "        tags = set()\n",
    "        for p in rec[\"pairs\"]:\n",
    "            tags.add(p[\"input_tag\"])\n",
    "            tags.add(p[\"output_tag\"])\n",
    "        for t in tags:\n",
    "            by_tag.setdefault(t, []).append(rec[\"id\"])\n",
    "    return by_tag, by_chap\n",
    "\n",
    "if 'dataset' in globals():\n",
    "    by_tag, by_chap = build_indexes(dataset)\n",
    "    with open(os.path.join(INDEX_DIR, \"by_tag.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(by_tag, f, ensure_ascii=False, indent=2)\n",
    "    with open(os.path.join(INDEX_DIR, \"by_chapter.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(by_chap, f, ensure_ascii=False, indent=2)\n",
    "    print(\"Indexes saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c746862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Simple query (v1.0 rules)\n",
    "def query(dataset: Dict[str,Any], inputs=None, outputs=None, chapters=None):\n",
    "    inputs = [t for t in (inputs or []) if t]\n",
    "    outputs = [t for t in (outputs or []) if t]\n",
    "    chapset = set([int(c) for c in chapters]) if chapters else None\n",
    "\n",
    "    results = []\n",
    "    for rec in dataset[\"data\"]:\n",
    "        # Chapter filter\n",
    "        chap = int(rec[\"ref\"].split()[1].split(\":\")[0])\n",
    "        if chapset and chap not in chapset:\n",
    "            continue\n",
    "\n",
    "        # Tag logic\n",
    "        verse_inputs = {p[\"input_tag\"] for p in rec[\"pairs\"]}\n",
    "        verse_outputs = {p[\"output_tag\"] for p in rec[\"pairs\"]}\n",
    "\n",
    "        if inputs and not set(inputs).issubset(verse_inputs):\n",
    "            continue\n",
    "        if outputs and not set(outputs).issubset(verse_outputs):\n",
    "            continue\n",
    "\n",
    "        results.append(rec)\n",
    "    return results\n",
    "\n",
    "def highlight(text:str, anchors:list):\n",
    "    htext = text\n",
    "    for a in anchors or []:\n",
    "        if a and a in htext:\n",
    "            htext = htext.replace(a, f\"<b><u><span>{a}</span></u></b>\")\n",
    "    return htext\n",
    "\n",
    "def render(results, translation=\"KJV\", limit=10):\n",
    "    rows = []\n",
    "    for rec in results[:limit]:\n",
    "        text = rec[\"text_KJV\"] if translation==\"KJV\" else (rec.get(\"text_WEB\") or rec[\"text_KJV\"])\n",
    "        # Use the first pair's anchors for preview; UI would show all\n",
    "        anchors = []\n",
    "        for p in rec[\"pairs\"]:\n",
    "            anchors.extend(p.get(\"anchor_phrases\",[])[:2])\n",
    "        h = highlight(text, anchors[:2])\n",
    "        pills = \" \".join([f\"<code>{p['input_tag']} → {p['output_tag']}</code>\" for p in rec[\"pairs\"]])\n",
    "        rows.append(f\"<div><strong>{rec['ref']}</strong><br/>{h}<br/>{pills}</div><hr/>\")\n",
    "    display(HTML(\"\\n\".join(rows) if rows else \"<i>No results.</i>\"))\n",
    "\n",
    "if 'dataset' in globals():\n",
    "    # Example: empty query shows all (first 10)\n",
    "    results = query(dataset, inputs=[], outputs=[], chapters=None)\n",
    "    render(results, translation=\"KJV\", limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cacfb1",
   "metadata": {},
   "source": [
    "> **Note:** BibleGateway multi-passage link builder and selection/bulk actions are deferred to v1.2. \n",
    "You can still prototype them here later in a separate section without affecting v1.0 data/output.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
